{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4341d8e2",
   "metadata": {},
   "source": [
    "# BRK441: Build and Launch AI Agents Fast\n",
    "## Speaker Guide & Delivery Notebook\n",
    "\n",
    "**Session Duration:** 40 minutes\n",
    "\n",
    "**Overview:** This notebook provides a comprehensive guide for delivering the BRK441 session, including slide-by-slide speaker notes, timing guidance, and relevant Microsoft documentation links.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd875d28",
   "metadata": {},
   "source": [
    "# Section 1: Welcome (0:00 - 3:04)\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Slide 1: Title Slide\n",
    "\n",
    "![Slide 1](slides/Slide1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0040a3",
   "metadata": {},
   "source": [
    "|**Speaker Notes** (20 seconds) |\n",
    "|---|\n",
    "|Hello everyone and welcome. My name is **SPEAKER NAME** and I am **SPEAKER ROLE**. Today I'm going to take you on a journey of how a developer prototyped, tested, and deployed a multimodal AI agent inside Visual Studio Code using extensions designed to streamline the creation of AI solutions.|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Introduce yourself warmly\n",
    "- Set the stage for the story-driven presentation\n",
    "- Mention the tools: Visual Studio Code + AI Toolkit\n",
    "- Emphasize the end-to-end journey (prototype ‚Üí test ‚Üí deploy)\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[AI Toolkit for VS Code](https://aka.ms/ai-toolkit)** - The official AI Toolkit extension for Visual Studio Code that enables rapid AI agent development\n",
    "- **[GitHub Copilot Documentation](https://docs.github.com/copilot)** - Learn about GitHub Copilot's capabilities for AI-assisted coding\n",
    "- **[Azure AI Documentation](https://learn.microsoft.com/azure/ai-services/)** - Comprehensive guide to Azure AI services and capabilities\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "The AI Toolkit is a VS Code extension that brings AI development capabilities directly into your editor. Think of it as your AI development workbench - it provides tools for choosing models, building agents, testing responses, and deploying to production, all without leaving your coding environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93e2bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 2: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f870074",
   "metadata": {},
   "source": [
    "![Slide 2](slides/Slide2.png)\n",
    "\n",
    "|**Speaker Notes** (50 seconds) |\n",
    "|---|\n",
    "|\"It's Saturday morning. Bruno Xiao stands in his apartment living room, coffee in one hand and his phone in the other, snapping a photo of the space he's about to transform. He's finally ready to paint. He knows the vibe he's going for - warm, cozy, maybe something with a little depth. But now he's staring at a bigger question: Eggshell or matte, satin or semi-gloss?\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Paint a vivid picture of Bruno's scenario\n",
    "- Make it relatable - we've all faced similar DIY decisions\n",
    "- Emphasize the challenge: finding the right finish\n",
    "- This isn't just about paint - it's about making confident decisions with limited expertise\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[AI Design Patterns](https://learn.microsoft.com/azure/architecture/ai-ml/)** - Common patterns for building AI applications\n",
    "- **[Multimodal AI Overview](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#multimodal-models)** - Learn about models that can process images and text together\n",
    "- **[Customer Scenario Planning](https://learn.microsoft.com/azure/architecture/guide/ai/)** - Guide to identifying good AI use cases\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "A multimodal AI agent can understand and process different types of input - like text AND images - together. In Bruno's case, the agent can see his living room photo while reading his questions about paint finishes, giving more contextually relevant recommendations than text-only solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd41106",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 3: Scenario - Bruno's Story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696f25a",
   "metadata": {},
   "source": [
    "![Slide 3](slides/Slide3.png)\n",
    "\n",
    "|**Speaker Notes** (27 seconds) |\n",
    "|---|\n",
    "|\"He wants the walls to feel elegant but still hide imperfections. He's not a pro. And one wrong decision could mean repainting everything next weekend. Zava, his local home DIY store, carries all the essentials. But finding the right finish, that's where things get tricky. And that's exactly where our story and our app begins.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Connect Bruno's pain point to the solution\n",
    "- Introduce Zava as the business context\n",
    "- Set up the technical challenge: navigating a large product catalog\n",
    "- Bridge from user story to developer story\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Conversational AI Best Practices](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message)** - How to design effective AI assistants\n",
    "- **[Retrieval Augmented Generation (RAG)](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data)** - Connect AI models to your business data\n",
    "- **[Responsible AI Guidelines](https://www.microsoft.com/ai/responsible-ai)** - Build trustworthy AI applications\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "The challenge here is connecting a powerful AI model (that knows general information) to specific business data (Zava's product catalog). This is where RAG comes in - it allows the AI to \"retrieve\" relevant product information and \"generate\" personalized recommendations based on both the customer's needs and available inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440ec2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 4: Meet Serena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263f29f",
   "metadata": {},
   "source": [
    "![Slide 4](slides/Slide4.png)\n",
    "\n",
    "|**Speaker Notes** (31 seconds) |\n",
    "|---|\n",
    "|\"Now, meet Serena. Serena's a developer at Zava, and her job is to help customers like Bruno get the right recommendations without needing to search for hours or stand in line at the paint counter. But she's got a constraint: limited time, a huge product catalog, and a business that needs results fast.\"|\n",
    "\n",
    "\"So, she builds Kora, a multimodal AI agent created and scaled with Microsoft Foundry. Kora isn't just chatty. Kora takes Bruno's input about his project and searches through Zava's catalog to find exactly what Bruno needs. Bruno can submit photos to Kora and Kora can process those photos as context when generating its product recommendations.\"\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Serena is our protagonist - a developer with real constraints\n",
    "- Emphasize the business pressure: limited time, need for results\n",
    "- Introduce Kora the agent - not just a chatbot, but a smart product assistant\n",
    "- Highlight multimodal capability: photos + text = better recommendations\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Microsoft Foundry Overview](https://learn.microsoft.com/azure/ai-studio/)** - Enterprise platform for building and deploying AI applications\n",
    "- **[Agent Development Guide](https://learn.microsoft.com/azure/ai-studio/how-to/develop-agents)** - Step-by-step guide to building AI agents\n",
    "- **[Model Context Protocol (MCP)](https://modelcontextprotocol.io/)** - Standard for connecting AI agents to external tools and data\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "Microsoft Foundry is like a complete development platform for AI applications. Instead of piecing together different tools and services, it provides everything you need in one place: access to AI models, tools for building agents, evaluation frameworks, and deployment capabilities. Think of it as \"Azure for AI\" - a unified platform specifically designed for generative AI workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68432ad0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 5: Session Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99686f",
   "metadata": {},
   "source": [
    "![Slide 5](slides/Slide5.png)\n",
    "\n",
    "|**Speaker Notes** (56 seconds) |\n",
    "|---|\n",
    "|\"And today I'm going to show you how Serena did it all within Visual Studio Code. In this session, we're going to explore developer-grade agent building from first prototype all the way to production.\"<br><br>**Session Overview - Cover These Points:**<br><br>1. **\"You'll see how Serena rapidly tested and compared Foundry models\"** - Model selection demo<br>2. **\"How she streamlined prompt engineering in the agent developer workflow with the agent builder in the AI toolkit\"** - Agent creation demo<br>3. **\"We'll also see how she evaluated agent responses using both manual testing and AI-assisted evaluators directly within the AI toolkit\"** - Evaluation demo<br>4. **\"And finally, we'll talk about what it takes to move from a working prototype to something that's production ready\"** - Deployment discussion<br><br>**\"Serena's story isn't just about building. It's about shipping smarter with the right tools at every step. And that's what today's session is all about.\"**|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is an end-to-end story about developer velocity and production readiness\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[AI Application Lifecycle](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-technical-paper)** - Understanding the full AI development process\n",
    "- **[Prompt Engineering Guide](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering)** - Techniques for crafting effective AI prompts\n",
    "- **[Testing AI Applications](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)** - Best practices for evaluating AI responses\n",
    "- **[Deploying AI Apps to Azure](https://learn.microsoft.com/azure/ai-studio/how-to/deploy-to-cloud)** - Production deployment strategies\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "Building production AI apps involves more than just getting a model to respond. You need to:\n",
    "1. **Choose the right model** (balancing cost, quality, and speed)\n",
    "2. **Design your agent** (writing effective prompts and connecting to your data)\n",
    "3. **Evaluate responses** (ensuring quality and catching issues before users do)\n",
    "4. **Deploy responsibly** (with monitoring, security, and scale in mind)\n",
    "\n",
    "This session follows all four steps in Serena's journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669071e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Set the Stage - Gen AI Ops (3:05 - 4:19)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 6: The Current State of AI Development\n",
    "\n",
    "![Slide 6](slides/Slide6.png)\n",
    "\n",
    "|**Speaker Notes** (37 seconds) |\n",
    "|---|\n",
    "|\"But let's zoom out for a second because Serena's not alone. Developers everywhere are building AI-powered apps right now from side projects to full-on production systems. And what we're seeing is this: The tooling for generative AI has exploded. There are more models, APIs, vector databases, and orchestration frameworks than ever before, which is exciting, but also overwhelming.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Acknowledge the current landscape: lots of choices = overwhelming\n",
    "- This is a universal developer experience, not just Serena's\n",
    "- The explosion of tools is both good and challenging\n",
    "- Set up the need for structure and best practices\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Azure AI Services Overview](https://learn.microsoft.com/azure/ai-services/)** - Understanding the breadth of available AI tools\n",
    "- **[Choosing AI Models](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog)** - Guidance for model selection\n",
    "- **[Vector Databases in Azure](https://learn.microsoft.com/azure/cosmos-db/vector-database)** - Storage solutions for AI applications\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "The AI ecosystem has grown rapidly. You might encounter:\n",
    "- **Models**: Different AI models (GPT-4, Claude, Llama, etc.) with varying capabilities\n",
    "- **Vector Databases**: Special databases for storing and searching AI embeddings\n",
    "- **Orchestration Frameworks**: Tools like LangChain, Semantic Kernel that help connect models to your app logic\n",
    "- **APIs**: Different ways to access AI capabilities\n",
    "\n",
    "It can feel overwhelming, but don't worry - you don't need to learn everything at once. Start with one model and one framework, then expand as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fb884",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 7: Gen AI Ops\n",
    "\n",
    "![Slide 7](slides/Slide7.png)\n",
    "\n",
    "|**Speaker Notes** (37 seconds) |\n",
    "|---|\n",
    "|\"Because here's the reality: Building an AI prototype is easy, but getting that prototype into production, that's the hard part. That's where the idea of Gen AI Ops comes in. Just like DevOps brought structure to traditional software development, Gen AI Ops is about bringing workflows, tools, and repeatability to how we build, test, and ship AI experiences.\"<br><br>\"In this session, you'll see what Gen AI Ops looks like in practice using tools from Microsoft Foundry and the AI toolkit in VS Code. You'll follow Serena's journey not just to build an agent, but to ship one that's useful, trusted, and scalable. So, let's dive in.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Draw the parallel to DevOps (audience will understand this)\n",
    "- Gen AI Ops = structure, workflows, repeatability for AI\n",
    "- We're not just building, we're shipping with quality\n",
    "- This is the foundation for everything that follows\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[MLOps for Generative AI](https://learn.microsoft.com/azure/machine-learning/concept-mlops)** - Applying software engineering practices to AI\n",
    "- **[AI DevOps Best Practices](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-maturity-model)** - Maturity model for AI operations\n",
    "- **[Continuous Integration for AI](https://learn.microsoft.com/azure/machine-learning/concept-continuous-integration)** - Testing and validation in AI pipelines\n",
    "- **[Monitoring AI Applications](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)** - Tracking performance in production\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Gen AI Ops** builds on DevOps principles but addresses unique AI challenges:\n",
    "\n",
    "| Traditional DevOps | Gen AI Ops |\n",
    "|-------------------|------------|\n",
    "| Code testing | Response quality testing |\n",
    "| Deployment | Model + prompt deployment |\n",
    "| Monitoring uptime | Monitoring accuracy, bias, costs |\n",
    "| Version control | Model + data versioning |\n",
    "\n",
    "Think of it as bringing the same rigor you apply to traditional software to AI applications - with testing, monitoring, and continuous improvement built into every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5c28b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Choose Your Model (4:20 - 12:20)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 8: AI Toolkit Introduction\n",
    "\n",
    "![Slide 8](slides/Slide8.png)\n",
    "\n",
    "|**Speaker Notes** (34 seconds) |\n",
    "|---|\n",
    "|\"The first real decision Serena faced wasn't tools or deployment. It was picking the right model to power Kora. And this is where the AI toolkit for Visual Studio Code comes in.\"<br><br>\"The AI toolkit is a full development environment right inside Visual Studio Code that gives you everything you need to explore models, build agents, evaluate responses, and deploy to Azure. It's built for developers who want to go beyond quick experiments and actually build, test, and ship AI-powered apps.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- AI Toolkit = your AI development workbench in VS Code\n",
    "- Complete environment: explore ‚Üí build ‚Üí evaluate ‚Üí deploy\n",
    "- Not just for prototyping - designed for production apps\n",
    "- This is your one-stop shop for the Gen AI Ops workflow\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[AI Toolkit Documentation](https://aka.ms/ai-toolkit/docs)** - Complete guide to using the AI Toolkit\n",
    "- **[Getting Started with AI Toolkit](https://learn.microsoft.com/windows/ai/toolkit/)** - Installation and first steps\n",
    "- **[AI Toolkit GitHub Repository](https://github.com/microsoft/vscode-ai-toolkit)** - Source code and samples\n",
    "- **[Video Tutorials](https://aka.ms/ai-toolkit/videos)** - Watch step-by-step guides\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "The **AI Toolkit extension** brings AI development capabilities into VS Code. After installing it, you'll find:\n",
    "\n",
    "1. **Model Catalog** - Browse and deploy AI models\n",
    "2. **Playground** - Test models with different prompts\n",
    "3. **Agent Builder** - Create agents with instructions and tools\n",
    "4. **Evaluation** - Test your agent's responses at scale\n",
    "5. **Code Export** - Generate production-ready code\n",
    "\n",
    "To install: Search for \"AI Toolkit\" in VS Code extensions, or visit [aka.ms/ai-toolkit](https://aka.ms/ai-toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70a9da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 9: Model Catalog\n",
    "\n",
    "![Slide 9](slides/Slide9.png)\n",
    "\n",
    "|**Speaker Notes** (96 seconds) |\n",
    "|---|\n",
    "|\"And at the core of that experience is the model catalog, your starting point for choosing and experimenting with models.\"<br><br>**Walk Through Model Options:**<br><br>1. **\"You can use GitHub hosted models which are free to use, no API key needed, and they're ideal for prototyping or just trying out agent logic. Although they are rate limited, you do have the option to leverage GitHub's pay as you go models.\"**<br><br>2. **\"There's also the Azure hosted models and these are for enterprise-grade apps where performance and scale and compliance matter.\"**<br><br>3. **\"We then have third-party APIs like OpenAI, Mistral, Cohere or anything you can hit with a REST endpoint.\"**<br><br>4. **\"And there's also local models including those running through Foundry local, Ollama or your own custom setup.\"**<br><br>**Key Message:** Flexibility - start free, scale to enterprise, use what works for you.|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Model catalog is the starting point for model selection\n",
    "- Four model hosting options to choose from\n",
    "- Start with free GitHub models for prototyping\n",
    "- Scale to Azure for enterprise needs\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[GitHub Models](https://github.com/marketplace/models)** - Free AI models for prototyping\n",
    "- **[Azure AI Model Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview)** - Enterprise-grade model deployment\n",
    "- **[Model as a Service](https://learn.microsoft.com/azure/ai-studio/how-to/deploy-models-serverless)** - Serverless API access to models\n",
    "- **[OpenAI on Azure](https://learn.microsoft.com/azure/ai-services/openai/)** - Deploying OpenAI models with Azure security\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "When choosing where to run your models, consider:\n",
    "\n",
    "| Option | Best For | Cost | Setup |\n",
    "|--------|----------|------|-------|\n",
    "| **GitHub Models** | Prototyping, learning | Free (rate limited) | No API key needed |\n",
    "| **Azure Models** | Production, enterprise | Pay-per-use | Requires Azure subscription |\n",
    "| **Third-party APIs** | Specific capabilities | Varies by provider | API keys required |\n",
    "| **Local Models** | Privacy, offline use | Free (your hardware) | More complex setup |\n",
    "\n",
    "**Start with GitHub Models** to experiment for free, then move to Azure when you're ready for production with enterprise security and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cce42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 10: GitHub Copilot Agent Mode\n",
    "\n",
    "![Slide 10](slides/Slide10.png)\n",
    "\n",
    "|**Speaker Notes** (96 seconds) |\n",
    "|---|\n",
    "|\"And if you aren't sure about which model to choose, then GitHub Copilot is here to help. GitHub Copilot Agent Mode is like having an autonomous peer programmer right inside your editor. In fact, GitHub Copilot Agent Mode can help with the complete agent building experience.\"<br><br>\"The AI toolkit embeds agent development workflows directly into Visual Studio Code and GitHub Copilot, enabling you to transform ideas into production ready agents within minutes. Equipped with several AI toolkit tools, GitHub Copilot agent mode can recommend models, build and orchestrate agents using the Microsoft agent framework, trace agent behavior, and evaluate agent responses for quality.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Copilot isn't just for writing code anymore\n",
    "- It understands the AI development workflow\n",
    "- Can recommend models based on your specific needs\n",
    "- Integrated with AI Toolkit for end-to-end assistance\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[GitHub Copilot Documentation](https://docs.github.com/copilot)** - Complete Copilot guide\n",
    "- **[Copilot Agent Mode](https://code.visualstudio.com/docs/copilot/copilot-chat#_chat-context)** - Using Copilot as an AI pair programmer\n",
    "- **[Custom Copilot Agents](https://code.visualstudio.com/docs/copilot/customization/custom-agents)** - Create specialized Copilot personas\n",
    "- **[Microsoft Agent Framework](https://learn.microsoft.com/semantic-kernel/)** - Framework for building AI agents\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**GitHub Copilot** has evolved beyond code completion. In **Agent Mode**, it can:\n",
    "\n",
    "- **Understand your project context** - Reads your files, dependencies, and code structure\n",
    "- **Recommend AI models** - Suggests models based on your use case\n",
    "- **Generate agent code** - Creates complete agent implementations\n",
    "- **Set up evaluations** - Helps design test cases for your agent\n",
    "- **Explain complex topics** - Answers questions about AI concepts\n",
    "\n",
    "Think of it as having an AI expert sitting next to you who knows both general AI best practices AND your specific project needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1907a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 11: DEMO - Model Recommendation\n",
    "\n",
    "![Slide 11](slides/Slide11.png)\n",
    "\n",
    "|**Speaker Notes** (96 seconds) |\n",
    "|---|\n",
    "|\"As for Serena, her first step was to ask GitHub Copilot for a model recommendation. And let's look at how she did that.\"<br><br>**Demo Flow (Show in VS Code):**<br><br>1. **Open GitHub Copilot Chat**<br>   - \"So here within Visual Studio Code, I'm going to open up GitHub Copilot chat.\"<br><br>2. **Ask for Recommendation**<br>   - Prompt: \"I'm creating an agent for a home improvement company. The agent uses the company's product catalog via an MCP server to recommend the right products for customers based on their DIY project. Which language model should I use?\"<br><br>3. **Review Copilot's Response**<br>   - \"GitHub Copilot is going to leverage one of the AI toolkit tools... the get AI model guidance tool\"<br>   - Review recommendation: GPT-4o mini<br>   - **Quality score** - Benchmarks<br>   - **Cost analysis** - Critical for production<br>   - **Latency** - User experience factor<br>   - **Context window** - How much data it can process<br>   - Budget alternative: GPT-4o nano<br>   - Free option: GitHub hosted GPT-4o mini<br><br>**Key Takeaway:** \"From here, the next thing that we would need to do is actually go test out this model and assess how well it performs against our given scenario.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - walk through the actual process\n",
    "- Emphasize how Copilot uses AI Toolkit tools to provide intelligent recommendations\n",
    "- Show the balance of quality, cost, and latency considerations\n",
    "- Mention the free GitHub-hosted option for getting started\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[GPT-4 Models Overview](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models)** - Understanding GPT-4 family capabilities\n",
    "- **[Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app)** - Comparing model performance\n",
    "- **[Azure OpenAI Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/)** - Understanding costs\n",
    "- **[Token Usage & Context Windows](https://learn.microsoft.com/azure/ai-services/openai/concepts/tokens)** - How models process input\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "When evaluating models, consider these key factors:\n",
    "\n",
    "**üéØ Quality**: How accurate and helpful are the responses?\n",
    "- Measured by benchmarks (MMLU, HumanEval, etc.)\n",
    "- Test with your specific use case\n",
    "\n",
    "**üí∞ Cost**: How much will it cost to run?\n",
    "- Input tokens (what you send to the model)\n",
    "- Output tokens (what the model generates)\n",
    "- Different models have different pricing\n",
    "\n",
    "**‚ö° Latency**: How fast does it respond?\n",
    "- Critical for user-facing applications\n",
    "- Affected by model size and complexity\n",
    "\n",
    "**üì¶ Context Window**: How much information can it process at once?\n",
    "- Larger context = more data can be analyzed\n",
    "- Important for complex tasks or long conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28030285",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 12: DEMO - Model Selection\n",
    "\n",
    "![Slide 12](slides/Slide12.png)\n",
    "\n",
    "|**Speaker Notes** (96 seconds) |\n",
    "|---|\n",
    "|**Demo Continues - Model Playground:**<br><br>1. **Open Model Catalog**<br>   - \"So to do that, we're going to open up the model catalog in the AI toolkit\"<br>   - Show variety of models available<br>   - Apply filters: \"supports image attachment\" (important for multimodal)<br><br>2. **Find and Deploy Models**<br>   - Search for GPT-4o mini<br>   - Choose Microsoft Foundry version (not GitHub or OpenAI for this demo)<br>   - \"Since Zava would like to develop agents for Microsoft Foundry...\"<br>   - Also add GPT-4o nano for comparison<br><br>3. **Test in Model Playground**<br>   - \"Within the model playground, this is where we can come and compare two models at a time\"<br>   - Select both models side by side<br>   - Test prompt: \"Describe what's in the image including colors of the objects\"<br>   - Attach Bruno's living room image<br>   - Submit once, populates both sides<br><br>4. **Review & Compare**<br>   - Check response quality<br>   - Note speed difference: \"4o mini was just a hair bit faster\"<br>   - \"What we would do now at this step if we were in Serena's shoes is just review the output, determine which of the two we like most, and then that's what we would move forward with.\"<br><br>**Key Outcome:** \"So at this point, Serena's chosen her model. She's tested it in the playground, compared outputs, and deployed a GPT-4o mini foundry model. And that sets the foundation for everything that comes next.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - actively demonstrate the Model Playground\n",
    "- Show the side-by-side comparison feature\n",
    "- Emphasize multimodal capabilities (image attachment)\n",
    "- Highlight how easy it is to test models before committing to code\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Model Playground Guide](https://learn.microsoft.com/azure/ai-studio/how-to/playground)** - Testing models interactively\n",
    "- **[Multimodal Capabilities](https://learn.microsoft.com/azure/ai-services/openai/how-to/gpt-with-vision)** - Using vision-enabled models\n",
    "- **[Prompt Engineering Techniques](https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering)** - Crafting effective prompts\n",
    "- **[Model Comparison Best Practices](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#compare-model-responses)** - A/B testing approaches\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "The **Model Playground** lets you test models before writing any code. Here's how to use it effectively:\n",
    "\n",
    "1. **Select models to compare** - Test up to 2 models side-by-side\n",
    "2. **Write a test prompt** - Use a real example from your use case\n",
    "3. **Check multimodal support** - Can it handle images, not just text?\n",
    "4. **Compare responses** - Look at quality, speed, and accuracy\n",
    "5. **Choose your winner** - Select the model that best fits your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050f7ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Design Your Agent üéØ DEMO 2 & 3 (12:21 - 19:52)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 13: Agent Builder Introduction\n",
    "\n",
    "![Slide 13](slides/Slide13.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|\"But models on their own aren't enough. To really help customers like Bruno, Serena needs to define how her agent should behave. What should it say and what should it know? And that's where the agent builder comes in. It's where Serena brings her agent to life. Let's walk through how she did it.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Models are just one piece - you need behavior and personality\n",
    "- Agent Builder = where you configure your agent's behavior\n",
    "- This is about more than prompts - it's about defining your agent's identity\n",
    "- Set up Demo 2\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[System Messages & Instructions](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message)** - Defining agent behavior\n",
    "- **[Agent Builder in AI Toolkit](https://learn.microsoft.com/windows/ai/toolkit/agent-builder)** - Using the visual agent designer\n",
    "- **[Prompt Engineering Best Practices](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering)** - Writing effective instructions\n",
    "- **[Persona Design for AI](https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering#persona)** - Creating consistent agent personalities\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "An **AI Agent** is more than just a model - it's a model plus:\n",
    "\n",
    "1. **System Instructions** (the \"brain\") - Tells the agent WHO it is, WHAT it does, and HOW to behave\n",
    "2. **Tools & Data Access** (the \"hands\") - Connects the agent to external systems like databases\n",
    "3. **Memory** (the \"context\") - Keeps track of conversation history\n",
    "4. **Guardrails** (the \"boundaries\") - Safety measures and scope limits\n",
    "\n",
    "Think of the Agent Builder as a form where you configure all these aspects visually, then it generates the code for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1166fe6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 14: DEMO - Creating Kora Agent\n",
    "\n",
    "![Slide 14](slides/Slide14.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|**Demo Flow - Creating Kora:**<br><br>1. **Open Agent Builder**<br>   - \"Here within the AI toolkit, I've opened up the agent builder\"<br><br>2. **Configure Basic Settings**<br>   - Name: \"Kora\"<br>   - Model: GPT-4o Mini from Microsoft Foundry<br><br>3. **System Prompt**<br>   - \"We can pass in a system prompt for the agent\"<br>   - Option: Generate with AI (describe task, AI generates prompt)<br>   - Serena already has the prompt ready<br>   - Read key parts: \"intelligent, friendly assistant... Zava brand... helps customers\"<br>   - Covers: **Role**, **Personality**, **Guidelines**<br><br>4. **Prompt Improvement Feature**<br>   - \"There's also a prompt improvement tool here that will suggest ways to make the prompt even better\"<br>   - Demo: Click \"Improve Prompt\" ‚Üí AI suggests improvements<br>   - Accept or reject suggestions<br>   - \"This is a really great way to refine the way your agent behaves\"|\n",
    "   - Show \"Improve\" button (appears after entering prompt)\n",
    "   - Can describe what to change, AI refines it\n",
    "   - Good for iterative development\n",
    "\n",
    "5. **Test Without Tools First**\n",
    "   - Submit test prompt: \"Here's a photo of my living room. I'm not sure whether I should go with eggshell or semi-gloss. Can you tell which would work better based on the lighting and layout?\"\n",
    "   - Upload Bruno's living room image\n",
    "   - Review response: Recommends eggshell, asks follow-ups\n",
    "   - **Problem identified**: \"What is missing is the fact that Kora does not yet have access to Zava's product catalog\"\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - walk through the Agent Builder interface\n",
    "- Show the system prompt components (role, personality, guidelines)\n",
    "- Demonstrate testing BEFORE adding tools\n",
    "- Key teaching moment: Agent can reason but can't access specific data yet\n",
    "- This sets up the need for MCP in the next section\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[System Messages & Instructions](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message)** - Defining agent behavior\n",
    "- **[Crafting Effective System Prompts](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message-framework)** - Framework for writing instructions\n",
    "- **[Testing AI Agents](https://learn.microsoft.com/azure/ai-studio/how-to/develop/test-agents)** - Best practices for agent testing\n",
    "- **[Iterative Prompt Engineering](https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering)** - Refining prompts over time\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "A good **System Prompt** should include:\n",
    "\n",
    "1. **Identity** - Who is the agent? (\"You are Kora, a helpful assistant for Zava Home Improvement\")\n",
    "2. **Purpose** - What does it do? (\"Help customers choose the right products for their projects\")\n",
    "3. **Personality** - How should it communicate? (\"Friendly, professional, encouraging\")\n",
    "4. **Guidelines** - What are the rules? (\"Always ask clarifying questions\", \"Use Zava brand voice\")\n",
    "5. **Constraints** - What should it NOT do? (\"Don't recommend competitors\", \"Stay within scope\")\n",
    "\n",
    "Start simple, then refine based on how the agent performs in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78185da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 15: Model Context Protocol (MCP)\n",
    "\n",
    "![Slide 15](slides/Slide15.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|\"And this is where MCP or Model Context Protocol comes into play. So what is model context protocol?\"<br><br>\"Think of MCP as a secured structured way for your agents to interact with external tools or services. Whether that's a database, a calendar, a weather API, or in Serena's case, a product catalog.\"<br><br>\"For Serena, this means that Kora can stay focused on the conversation while the heavy lifting like looking up products is delegated to a tool behind the scenes. And because it's all orchestrated through MCP, Kora doesn't need to know how the product catalog works, Kora just needs to know that there's a 'get products by name' tool that can be called.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- MCP = standardized way to connect agents to external tools\n",
    "- Think of it like an API for AI agents\n",
    "- Separation of concerns: Agent handles conversation, tools handle data\n",
    "- Agent doesn't need to understand database queries - just calls a tool by name\n",
    "- This is a concept slide - explain the \"why\" before showing the \"how\"\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[MCP Official Documentation](https://modelcontextprotocol.io/)** - Complete MCP specification and guides\n",
    "- **[MCP in AI Toolkit](https://learn.microsoft.com/windows/ai/toolkit/mcp-integration)** - Using MCP with AI Toolkit\n",
    "- **[Building MCP Servers](https://modelcontextprotocol.io/docs/concepts/servers)** - Creating custom tools\n",
    "- **[MCP GitHub Repository](https://github.com/modelcontextprotocol)** - Examples and community servers\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Model Context Protocol (MCP)** is an open standard that lets AI agents connect to external tools and data sources in a consistent way.\n",
    "\n",
    "**Why MCP Matters:**\n",
    "\n",
    "| Without MCP | With MCP |\n",
    "|------------|----------|\n",
    "| Each agent needs custom code for each tool | Standardized interface for all tools |\n",
    "| Hard to reuse tools across projects | Write once, use everywhere |\n",
    "| Security and permissions are ad-hoc | Built-in security model |\n",
    "| Testing is complex | Tools can be tested independently |\n",
    "\n",
    "**Real-world example**: Instead of writing custom code to query your database, you create an MCP server that exposes \"search_products\" as a tool. Any agent can then use that tool without knowing anything about your database structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b6e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 16: DEMO - Adding MCP Tools\n",
    "\n",
    "![Slide 16](slides/Slide16.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|\"So let's take a look at how Serena connected Kora to Zava's MCP server which is backed by a Postgres SQL product catalog.\"<br><br>**Demo Flow - Adding MCP Tools:**<br><br>1. **Explain Zava's MCP Server**<br>   - \"Back here in the agent builder I can connect to Zava's custom MCP server\"<br>   - Features:<br>     * Product searches by name with fuzzy matching<br>     * Store-specific product availability (row-level security)<br>     * Real-time inventory levels and stock information<br><br>2. **Show Running Server**<br>   - \"I happen to already have the server running here in the background within VS Code\"<br>   - Can access via agent builder<br><br>3. **Add Tools to Agent**<br>   - Scroll to Tools section<br>   - Click \"Add the tools via the MCP server option\"<br>   - Options shown:<br>     * Use tools already added in VS Code ‚úì<br>     * Browse available servers<br>     * Manually add servers<br>     * Create your own servers with AI Toolkit<br><br>4. **Select Specific Tools**<br>   - \"I only need in this case the get products by name\"<br>   - Select and click OK<br>   - Tool is now added to Kora<br><br>5. **Test with Tools**<br>   - Start new chat (important for fresh context)<br>   - Same prompt as before + Bruno's image<br>   - Wait for response...<br>   - **Look for tool call indicator in UI**<br>   - Response now includes: Interior Eggshell Paint from Zava + price + availability<br><br>**Key Win:** \"So now that Kora is up and running and connected to Zava's product catalog using MCP, Serena has a working prototype.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - actively demonstrate adding MCP tools\n",
    "- Show the tool selection UI in Agent Builder\n",
    "- Point out the tool call indicator when agent uses the tool\n",
    "- Contrast this result with the earlier test (no specific products)\n",
    "- Emphasize how easy it is to add MCP tools\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Creating MCP Servers](https://modelcontextprotocol.io/docs/tutorials/building-mcp-with-llms)** - Build your first MCP server\n",
    "- **[MCP Server Registry](https://github.com/modelcontextprotocol/servers)** - Pre-built community servers\n",
    "- **[Testing MCP Tools](https://modelcontextprotocol.io/docs/tools/inspector)** - MCP Inspector for debugging\n",
    "- **[Row-Level Security in PostgreSQL](https://www.postgresql.org/docs/current/ddl-rowsecurity.html)** - Data access controls\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "When adding MCP tools to your agent:\n",
    "\n",
    "1. **Tool Discovery** - Agent Builder shows all available tools from connected MCP servers\n",
    "2. **Selective Addition** - Choose only the tools your agent needs (keeps context focused)\n",
    "3. **Tool Descriptions** - MCP servers provide descriptions so the agent knows when to use each tool\n",
    "4. **Automatic Orchestration** - Agent decides when to call tools based on user requests\n",
    "5. **Testing** - Always test with real queries after adding tools to verify behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8612a12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 17: Testing the Agent\n",
    "\n",
    "![Slide 17](slides/Slide17.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|\"But before she ships it, she needs to know: Is Kora actually doing what Kora is supposed to do? And are the responses clear? Are they trustworthy? And are they actually helpful for Zava's customers? Essentially, Serena wants to know, can she trust this Kora agent to interact with real customers like Bruno?\"<br><br>\"And this is where evaluation comes in.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Working prototype ‚â† production-ready agent\n",
    "- Need to validate: accuracy, trustworthiness, helpfulness\n",
    "- Can't just ship and hope for the best\n",
    "- This is where evaluation becomes critical\n",
    "- Set up the transition to the evaluation section\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Testing AI Applications](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)** - Comprehensive testing strategies\n",
    "- **[Quality Assurance for AI](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app)** - QA best practices\n",
    "- **[Responsible AI Testing](https://learn.microsoft.com/azure/ai-services/openai/concepts/safety-guidelines#testing-and-monitoring)** - Safety and ethics validation\n",
    "- **[AI Red Teaming](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming)** - Stress-testing AI systems\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "Before deploying an AI agent to production, ask these critical questions:\n",
    "\n",
    "**‚úÖ Accuracy**\n",
    "- Does it provide correct information?\n",
    "- Are product recommendations appropriate?\n",
    "- Does it hallucinate (make up information)?\n",
    "\n",
    "**‚úÖ Trustworthiness**\n",
    "- Does it cite sources when appropriate?\n",
    "- Does it admit when it doesn't know something?\n",
    "- Is it consistent across similar questions?\n",
    "\n",
    "**‚úÖ Helpfulness**\n",
    "- Does it actually solve the user's problem?\n",
    "- Is the tone appropriate for the audience?\n",
    "- Does it ask good follow-up questions?\n",
    "\n",
    "**‚úÖ Safety**\n",
    "- Does it refuse inappropriate requests?\n",
    "- Does it protect sensitive information?\n",
    "- Does it avoid biased or harmful responses?\n",
    "\n",
    "You can't test these things with just a few manual interactions - you need systematic evaluation at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7e3d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 18: Agent Capabilities\n",
    "\n",
    "![Slide 18](slides/Slide18.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|**Quick Recap Before Moving to Evaluation:**<br>- Kora can understand images (multimodal)<br>- Kora can access product catalog (MCP tools)<br>- Kora provides natural, helpful responses<br>- Ready for evaluation phase|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Summarize what Kora can do at this point\n",
    "- Emphasize the multimodal + MCP combination\n",
    "- This is a transition slide - keep it brief\n",
    "- Set up audience expectations for evaluation section\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Multimodal AI Applications](https://learn.microsoft.com/azure/ai-services/openai/how-to/gpt-with-vision)** - Building vision-enabled agents\n",
    "- **[Tool Use Patterns](https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling)** - Best practices for tool integration\n",
    "- **[Conversation Management](https://learn.microsoft.com/azure/ai-services/openai/how-to/chat-completions)** - Handling multi-turn dialogues\n",
    "- **[Agent Architecture Patterns](https://learn.microsoft.com/azure/architecture/ai-ml/architecture/baseline-openai-e2e-chat)** - Reference architectures\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "A complete AI agent typically has these capabilities:\n",
    "\n",
    "üéØ **Understanding** - Process text, images, and other inputs  \n",
    "üîß **Action** - Use tools to fetch data or perform tasks  \n",
    "üí¨ **Communication** - Respond naturally and helpfully  \n",
    "üß† **Reasoning** - Decide when and how to use available tools  \n",
    "üìù **Memory** - Maintain context across conversation turns\n",
    "\n",
    "Kora now has all five of these capabilities, making it ready for real-world testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa799bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 19: Agent Development Summary\n",
    "\n",
    "![Slide 19](slides/Slide19.png)\n",
    "\n",
    "|**Speaker Notes** (64 seconds) |\n",
    "|---|\n",
    "|**Key Achievements So Far:**<br>- ‚úÖ Selected the right model (GPT-4o mini)<br>- ‚úÖ Designed Kora's personality and behavior<br>- ‚úÖ Connected to Zava's product catalog via MCP<br>- ‚úÖ Tested basic functionality<br><br>**Transition:** \"Now Serena has a working prototype. But before she ships it, she needs to know it's actually doing what it's supposed to do. That's where evaluation comes in.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a summary/transition slide\n",
    "- Recap the three main steps completed\n",
    "- Emphasize \"working prototype\" vs \"production-ready\"\n",
    "- Build anticipation for evaluation section\n",
    "- Keep it concise and energetic\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[End-to-End Agent Tutorial](https://learn.microsoft.com/azure/ai-studio/tutorials/deploy-chat-web-app)** - Complete walkthrough\n",
    "- **[From Prototype to Production](https://learn.microsoft.com/azure/architecture/ai-ml/guide/machine-learning-operations-v2)** - Production readiness checklist\n",
    "- **[Agent Design Patterns](https://learn.microsoft.com/azure/architecture/ai-ml/)** - Common architectural patterns\n",
    "- **[AI Project Lifecycle](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-technical-paper)** - Full development process\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "You've now seen the complete agent development process:\n",
    "\n",
    "**Phase 1: Model Selection** ‚Üí Choose the right AI model for your needs  \n",
    "**Phase 2: Agent Design** ‚Üí Define behavior, personality, and capabilities  \n",
    "**Phase 3: Tool Integration** ‚Üí Connect to data and external systems  \n",
    "\n",
    "**But development isn't done yet!** The most critical phase is coming next:\n",
    "\n",
    "**Phase 4: Evaluation** ‚Üí Validate quality before deployment  \n",
    "\n",
    "Many teams skip this step and ship agents without proper testing. Don't be one of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ee560",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Evaluate Your Responses üéØ DEMO 4 (19:53 - 31:15)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 20: Importance of Evaluation\n",
    "\n",
    "![Slide 20](slides/Slide20.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|\"Evaluation is a critical part of Gen AI Ops because in the real world, users aren't grading on a curve. If your agent gives bad recommendations, users lose trust fast.\"<br><br>\"Serena uses the AI toolkit to evaluate responses in two ways:\"<br><br>**1. Manually** - \"where she looks at sample prompts and responses to check for accuracy, tone, and hallucinations\"<br><br>**2. AI-Assisted** - \"and that's where the AI toolkit runs evaluators like task adherence and relevance across a test set of queries. And that scores the agent's performance at scale\"<br><br>**Key Message:**<br>- Users won't forgive bad responses<br>- Need both human judgment (manual) and scale (AI-assisted)<br>- Catch issues before going live<br>- Build trust through systematic evaluation<br><br>\"These tools help Serena catch issues early. She can also iterate faster and build trust in the output all before going live. So, let me show you how Serena evaluates Kora inside the AI toolkit.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Emphasize the stakes - bad responses = lost trust\n",
    "- Explain the two-pronged approach (manual + AI-assisted)\n",
    "- Manual = quality, AI-assisted = scale\n",
    "- This is foundational to Gen AI Ops\n",
    "- Set up Demo 4\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Evaluation Overview](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)** - Why and how to evaluate AI\n",
    "- **[Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk)** - Programmatic evaluation\n",
    "- **[Built-in Evaluators](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)** - Pre-built evaluation metrics\n",
    "- **[Custom Evaluators](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#custom-evaluators)** - Create your own metrics\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Why Two Types of Evaluation?**\n",
    "\n",
    "| Manual Evaluation | AI-Assisted Evaluation |\n",
    "|-------------------|------------------------|\n",
    "| **Human reviews responses** | **AI scores responses automatically** |\n",
    "| Catches nuanced issues | Scales to hundreds/thousands of tests |\n",
    "| Subjective quality judgment | Objective metrics |\n",
    "| Time-consuming | Fast and repeatable |\n",
    "| Best for: Edge cases, tone, creativity | Best for: Consistency, factual accuracy |\n",
    "\n",
    "**Best Practice**: Start with manual evaluation on 10-20 representative examples, then use AI-assisted evaluation to scale testing across your entire dataset.\n",
    "\n",
    "**Common Evaluation Metrics:**\n",
    "- **Relevance**: Does the response address the question?\n",
    "- **Coherence**: Is the response well-structured and logical?\n",
    "- **Groundedness**: Is the response based on provided data (not hallucinated)?\n",
    "- **Fluency**: Is the response well-written and natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b14b3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 21: DEMO - Manual Evaluation\n",
    "\n",
    "![Slide 21](slides/Slide21.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|**Demo Flow - Evaluation Tab:**<br><br>1. **Switch to Evaluation Tab**<br>   - In Agent Builder, click Evaluation tab<br><br>2. **Show Generate Data Feature** (starry icon)<br>   - \"In the event you want to evaluate your agent, but you don't yet have data, we can generate that data for you\"<br>   - Specify number of rows<br>   - Uses agent instructions to generate relevant test cases<br><br>3. **Add Manual Test Data**<br>   - \"I happen to actually already have the data that Serena used\"<br>   - Click \"add an empty row\"<br>   - Example queries:<br>     * \"What type of organic compost does Zava have?\"<br>     * \"Does Zava have a paint bucket? If so, how much is it?\"<br>     * \"What color glitter does Zava sell?\" (trick question - they don't!)<br>     * \"How much tape measure is currently in stock?\"<br><br>4. **Run Agent Responses**<br>   - Can run individually or all at once<br>   - Click \"Run response\"<br>   - Watch for tool calls (should see them for product queries)<br><br>5. **Manual Evaluation with Thumbs**<br>   - Review each response<br>   - Row 1 (compost): ‚úÖ Correct product info ‚Üí Thumbs up<br>   - Row 2 (paint bucket): ‚ùå Recommended paint tray instead ‚Üí Thumbs down<br>   - Row 3 (glitter): ‚úÖ Correctly says \"not available\" ‚Üí Thumbs up<br>   - Row 4 (tape measure stock): ‚úÖ Accurate inventory count ‚Üí Thumbs up<br><br>**Key Teaching:** Manual evaluation helps you catch subtle issues - like recommending a paint tray when user asked for a bucket.|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - actively demonstrate the Evaluation tab\n",
    "- Show how to add test data (both generate and manual entry)\n",
    "- Run the agent and point out tool calls\n",
    "- Use thumbs up/down to demonstrate manual scoring\n",
    "- Emphasize the \"trick question\" (glitter) - tests edge cases\n",
    "- Point out the paint bucket issue - shows importance of human review\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Human Evaluation Guidelines](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#manual-evaluation)** - Structured human review\n",
    "- **[Test Data Design](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#prepare-your-test-data)** - Creating effective test cases\n",
    "- **[Edge Case Testing](https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering#test-edge-cases)** - Testing unusual scenarios\n",
    "- **[Red Team Testing](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming)** - Adversarial testing approaches\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Creating Good Test Cases:**\n",
    "\n",
    "1. **Representative Scenarios** - Real questions your users will ask\n",
    "2. **Edge Cases** - Unusual or tricky requests\n",
    "3. **Expected Failures** - Questions agent should refuse or clarify\n",
    "4. **Varied Complexity** - Simple to complex queries\n",
    "\n",
    "**Example Test Set for Kora:**\n",
    "```\n",
    "‚úÖ Simple product lookup: \"Does Zava have paint?\"\n",
    "‚úÖ Specific request: \"I need eggshell finish paint\"\n",
    "‚úÖ Image + question: [living room photo] + \"What paint do you recommend?\"\n",
    "‚ùå Out of scope: \"Can you paint my living room for me?\"\n",
    "‚ùå Product not available: \"Do you sell glitter?\" ‚Üí Should say \"no\"\n",
    "‚ùå Unclear request: \"I need stuff for walls\" ‚Üí Should ask clarifying questions\n",
    "```\n",
    "\n",
    "**Pro Tip**: Include \"trick questions\" like asking for products you don't sell - this tests whether your agent can gracefully handle missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17ecc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 22: AI-Assisted Evaluation\n",
    "\n",
    "![Slide 22](slides/Slide22.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|\"We can also have AI assess the agent output as well, and that will be an AI-assisted evaluation.\"<br><br>**Demo Flow - Adding AI Evaluators:**<br><br>1. **Ask Copilot for Help** (if unsure which evaluators to use)<br>   - Open Copilot chat<br>   - \"Which evaluators do you recommend that I should use to do evaluations for my agent?\"<br>   - Copilot invokes \"evaluation planner\" tool<br>   - Can take project context into account<br>   - Recommends: **Relevance** and **Coherence** evaluators<br><br>2. **Add Evaluators in UI**<br>   - Back in evaluation tab<br>   - Click \"Add evaluation\"<br>   - Select: Relevance and Coherence<br>   - Click OK<br><br>3. **Choose Judge Model**<br>   - Select model to do the evaluation: GPT-4o Mini<br>   - \"The language model that I want to use as the AI that does these AI-assisted evaluations\"<br><br>4. **New Columns Appear**<br>   - Relevance and Coherence columns added to grid<br>   - Can run all at once or individually<br><br>5. **Run Evaluation**<br>   - Click \"Run evaluation only\" for first row<br>   - Assesses the organic compost response<br>   - View inline or click row for detailed view<br><br>6. **Review Results**<br>   - Relevance: 4 out of 5<br>   - Coherence: 4 out of 5<br>   - Includes reasoning for each score<br>   - Shows model response, tool response, final output<br><br>**Note:** \"When you complete evaluations with the Azure AI evaluation SDK, you do also receive the relevance reason, you get the coherence reason, and you'll get the reasoning for the other built-in evaluators as well.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - show AI-assisted evaluation in action\n",
    "- Highlight Copilot's evaluation planner tool\n",
    "- Demonstrate how to add evaluators in the UI\n",
    "- Show the detailed view with reasoning\n",
    "- Emphasize the scale advantage (can run on 100s of queries)\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Azure AI Evaluation SDK](https://learn.microsoft.com/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview)** - Python SDK for evaluation\n",
    "- **[Evaluator Metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)** - Understanding each metric\n",
    "- **[Evaluation Best Practices](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#best-practices)** - Guidelines for effective evaluation\n",
    "- **[Custom Evaluators](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#custom-evaluators)** - Building your own metrics\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Built-in Evaluators Explained:**\n",
    "\n",
    "**1. Relevance (1-5 scale)**\n",
    "- Does the response actually answer the user's question?\n",
    "- Scores: 1=Off topic, 3=Partially relevant, 5=Perfectly relevant\n",
    "\n",
    "**2. Coherence (1-5 scale)**\n",
    "- Is the response logical, well-organized, and easy to follow?\n",
    "- Scores: 1=Confusing, 3=Mostly clear, 5=Crystal clear\n",
    "\n",
    "**3. Groundedness**\n",
    "- Is information based on provided context (not made up)?\n",
    "- Critical for preventing hallucinations\n",
    "\n",
    "**4. Fluency**\n",
    "- Is the language natural and grammatically correct?\n",
    "- Sounds like a human wrote it\n",
    "\n",
    "**5. Custom Evaluators**\n",
    "- You can create your own for specific needs\n",
    "- Example: \"Does response include price?\"\n",
    "\n",
    "**How It Works:**\n",
    "1. Your agent generates a response\n",
    "2. A \"judge\" model (like GPT-4) evaluates that response\n",
    "3. Provides score + reasoning\n",
    "4. You review results and iterate\n",
    "\n",
    "**Pro Tip**: The judge model should be equal or better quality than your agent model for accurate evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c007c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 23: Evaluator Results\n",
    "\n",
    "![Slide 23](slides/Slide23.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|**Highlight Key Insights:**<br>- Can view results inline in grid<br>- Click row for detailed view with all context<br>- See the full chain: Input ‚Üí Tool Call ‚Üí Model Response ‚Üí Final Output<br>- Reasoning helps you understand why scores were given<br>- Use this to identify patterns and improvement opportunities|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Show the detailed results view in the demo\n",
    "- Point out the reasoning text that explains scores\n",
    "- Emphasize that you get full transparency into the evaluation\n",
    "- This is what makes iteration possible\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Understanding Metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in#interpreting-results)** - What scores mean\n",
    "- **[Result Analysis](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#analyze-evaluation-results)** - Finding patterns\n",
    "- **[Improving Performance](https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering#improve-performance)** - Acting on insights\n",
    "- **[Evaluation Dashboards](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#visualize-results)** - Tracking metrics over time\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Reading Evaluation Scores:**\n",
    "\n",
    "**Score: 4-5** ‚úÖ Excellent - Agent is performing well  \n",
    "**Score: 3** ‚ö†Ô∏è Acceptable - Room for improvement  \n",
    "**Score: 1-2** ‚ùå Poor - Needs attention\n",
    "\n",
    "**What to Do with Results:**\n",
    "- **Low Relevance** ‚Üí Improve system prompt, add examples\n",
    "- **Low Coherence** ‚Üí Simplify instructions, restructure responses  \n",
    "- **Low Groundedness** ‚Üí Add citations, improve RAG pipeline\n",
    "- **Inconsistent Scores** ‚Üí Test with more data, refine prompts\n",
    "\n",
    "Look for patterns across multiple test cases, not individual outliers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece828af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 24: Evaluation Best Practices\n",
    "\n",
    "![Slide 24](slides/Slide24.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|\"Evaluating agents is only effective if your comparisons are well-structured. Random side-by-sides won't reveal what's actually better. You need a consistent, thoughtful approach to how you test.\"<br><br>**Best Practices for Effective Evaluation:**<br><br>1. **\"Change one variable at a time\"**<br>   - \"If version A uses GPT-4 and version B uses GPT-4o and the new tool and the new prompt, you won't know which change made the difference\"<br>   - Isolate variables to understand impact<br><br>2. **\"Use the same test prompts\"**<br>   - \"Run each version of your agent against the same scenarios, ideally ones that reflect real user needs or known edge cases\"<br><br>3. **\"Evaluate against the same criteria\"**<br>   - Manual scoring or automated metrics - keep rubric consistent<br>   - \"Compare apples to apples\"<br><br>4. **\"Keep notes or a change log\"**<br>   - \"Even a simple note like 'V3 adds retrieval tool and uses a shorter prompt'\"<br>   - Makes it easier to revisit decisions<br><br>5. **\"Watch for trade-offs\"**<br>   - \"Sometimes one version improves relevance but loses fluency or speeds up performance but drops grounding\"<br>   - Surface tensions, prioritize intentionally<br><br>**Key Message:** \"The goal isn't just to find the best version. It's to understand why one works better than another. And that insight is what makes your agent stronger with every iteration.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a teaching slide - emphasize the scientific approach\n",
    "- Stress the \"one variable at a time\" principle\n",
    "- Explain that random testing wastes time\n",
    "- These practices apply beyond AI - good engineering discipline\n",
    "- Sets up the mindset for continuous improvement\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[A/B Testing for AI](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-technical-paper#model-validation)** - Comparing versions\n",
    "- **[Experimentation Framework](https://learn.microsoft.com/azure/machine-learning/concept-mlflow)** - Tracking experiments\n",
    "- **[Version Control for AI](https://learn.microsoft.com/azure/machine-learning/concept-model-management-and-deployment)** - Managing iterations\n",
    "- **[MLOps Maturity Model](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-maturity-model)** - Progression to production\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Scientific Approach to AI Testing:**\n",
    "\n",
    "| ‚ùå Random Testing | ‚úÖ Structured Testing |\n",
    "|------------------|----------------------|\n",
    "| \"Let's try GPT-4 instead\" | \"Hypothesis: GPT-4 will improve accuracy\" |\n",
    "| Change everything at once | Change one thing at a time |\n",
    "| Informal notes | Systematic change log |\n",
    "| \"It feels better\" | Measure with consistent metrics |\n",
    "\n",
    "**Example Change Log:**\n",
    "```\n",
    "V1 (Baseline): GPT-4o mini, basic prompt, no tools\n",
    "  - Relevance: 3.2, Coherence: 3.5\n",
    "\n",
    "V2: Same as V1 + Added MCP product search\n",
    "  - Relevance: 4.5 ‚¨ÜÔ∏è, Coherence: 3.5 ‚û°Ô∏è\n",
    "  - Conclusion: Tool improved relevance significantly\n",
    "\n",
    "V3: V2 + Refined system prompt (added personality)\n",
    "  - Relevance: 4.5 ‚û°Ô∏è, Coherence: 4.2 ‚¨ÜÔ∏è\n",
    "  - Conclusion: Better personality improves coherence\n",
    "```\n",
    "\n",
    "**This structured approach helps you:**\n",
    "- Know what changes actually helped\n",
    "- Avoid making things worse accidentally\n",
    "- Build institutional knowledge\n",
    "- Make data-driven decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e68e56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 25: Iterative Improvement\n",
    "\n",
    "![Slide 25](slides/Slide25.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|**Key Points:**<br>- Evaluation isn't a one-time activity<br>- Use insights to refine prompts, adjust tools, tune model selection<br>- Each iteration should be measured against previous versions<br>- Build confidence through systematic improvement|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Brief slide - emphasize the cycle of continuous improvement\n",
    "- Evaluation leads to insights ‚Üí insights lead to changes ‚Üí changes need re-evaluation\n",
    "- This is the heart of Gen AI Ops\n",
    "- Never stop improving\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Iterative Development](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-generative-ai-app#iterate-and-improve)** - Refining your agent\n",
    "- **[Feedback Loops](https://learn.microsoft.com/azure/architecture/ai-ml/guide/mlops-maturity-model)** - Learning from production\n",
    "- **[Model Monitoring](https://learn.microsoft.com/azure/machine-learning/concept-model-monitoring)** - Tracking performance over time\n",
    "- **[Continuous Deployment](https://learn.microsoft.com/azure/machine-learning/how-to-deploy-continuously-deploy)** - Automated updates\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**The AI Development Cycle:**\n",
    "\n",
    "1. **Build** ‚Üí Create initial agent\n",
    "2. **Evaluate** ‚Üí Test with representative data\n",
    "3. **Analyze** ‚Üí Identify what needs improvement\n",
    "4. **Refine** ‚Üí Make targeted changes\n",
    "5. **Re-evaluate** ‚Üí Measure impact\n",
    "6. **Repeat** ‚Üí Continue until quality targets met\n",
    "\n",
    "This cycle never truly ends - even in production, you should continuously monitor and improve based on real user feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3f655",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 26: Evaluation Summary\n",
    "\n",
    "![Slide 26](slides/Slide26.png)\n",
    "\n",
    "|**Speaker Notes** (97 seconds) |\n",
    "|---|\n",
    "|\"All right, so back to Serena and the Kora agent. At this stage, Serena has validated that Kora is behaving as expected. She's tested the responses, tuned the prompts, and validated the output.\"<br><br>**Evaluation Complete:**<br>- ‚úÖ Manual testing done - human review passed<br>- ‚úÖ AI-assisted evaluation done - metrics look good<br>- ‚úÖ Confidence established - ready for next phase<br><br>**Next Challenge:** \"Now the question is, how does she get this agent into production? How does she move from a working prototype in the AI toolkit to something customers can actually use?\"|\n",
    "- ‚úÖ Iterative improvements made based on findings\n",
    "- ‚úÖ Confidence built through systematic testing\n",
    "\n",
    "**Transition to Deployment:** \"But a working agent isn't the same as a working app. Next, Serena needed to integrate the agent logic into a real-world product and get it deployed.\"\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Recap what was accomplished in this section\n",
    "- Emphasize validation and confidence building\n",
    "- Clear transition: agent works, now needs to become an app\n",
    "- Set up Section 6 (deployment)\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Production Readiness Checklist](https://learn.microsoft.com/azure/architecture/ai-ml/guide/machine-learning-operations-v2#production-readiness-checklist)** - What to verify before deploying\n",
    "- **[Quality Gates](https://learn.microsoft.com/azure/machine-learning/concept-mlops#quality-gates)** - Setting minimum standards\n",
    "- **[Deployment Planning](https://learn.microsoft.com/azure/ai-studio/concepts/deployments-overview)** - Preparing for production\n",
    "- **[Go-Live Checklist](https://learn.microsoft.com/azure/well-architected/)** - Azure Well-Architected Framework\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Production Readiness Criteria:**\n",
    "\n",
    "‚úÖ **Quality Validated** - Evaluation scores meet targets  \n",
    "‚úÖ **Edge Cases Tested** - Unusual inputs handled gracefully  \n",
    "‚úÖ **Safety Verified** - Inappropriate requests refused  \n",
    "‚úÖ **Performance Acceptable** - Response time within limits  \n",
    "‚úÖ **Cost Estimated** - Usage costs understood and budgeted  \n",
    "‚úÖ **Monitoring Planned** - Know how you'll track production behavior\n",
    "\n",
    "Only when ALL criteria are met should you proceed to deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1177a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Deploy Your Solution üéØ DEMO 5 (31:16 - 38:35)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 27: Code Export & Integration\n",
    "\n",
    "![Slide 27](slides/Slide27.png)\n",
    "\n",
    "|**Speaker Notes** (110 seconds) |\n",
    "|---|\n",
    "|\"For now, let's focus on getting the agent into a real-world product. One of Serena's colleagues created the UI for the Kora app and passed along the project files. Therefore, the front end is complete, but what's missing is the brain behind it, the agent logic, the call to the MCP server, and the model responses.\"<br><br>**Demo Flow - Code Export:**<br><br>1. **Show Code Export Feature**<br>   - \"Within the AI toolkit, Serena can export the agent code via her preferred SDK\"<br>   - Scroll to bottom of Agent Builder<br>   - Click \"View Code\"<br><br>2. **Three SDK Options Shown:**<br>   - Azure AI Inference SDK<br>   - Semantic Kernel SDK<br>   - Microsoft Agent Framework ‚úì (Serena's choice)<br><br>3. **Select Framework**<br>   - \"Serena chose to go with the Microsoft agent framework in which Python was her chosen language\"<br>   - Shows complete agent code file<br><br>4. **Review Generated Code**<br>   - Includes system prompt<br>   - Model configuration<br>   - MCP tool connections<br>   - Everything configured in Agent Builder is now code<br><br>5. **Export to Project**<br>   - Can copy code directly<br>   - Or export as file to integrate into existing app<br><br>**Key Point:** \"All the work done in the visual Agent Builder translates directly into production-ready code.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is a DEMO slide - show the code export feature\n",
    "- Emphasize that everything visual becomes code\n",
    "- Three SDK options give flexibility\n",
    "- Code is production-ready, not just a prototype\n",
    "- This bridges the gap between design and development\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Microsoft Agent Framework](https://learn.microsoft.com/python/api/overview/azure/ai-agent-framework)** - Python framework for agents\n",
    "- **[Azure AI Inference SDK](https://learn.microsoft.com/azure/ai-services/openai/how-to/python-sdk)** - Direct model access\n",
    "- **[Semantic Kernel](https://learn.microsoft.com/semantic-kernel/)** - Multi-language orchestration framework\n",
    "- **[Code Export Documentation](https://learn.microsoft.com/windows/ai/toolkit/agent-builder#export-code)** - Using exported code\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Three SDK Options Explained:**\n",
    "\n",
    "**1. Azure AI Inference SDK**\n",
    "- Direct model API calls\n",
    "- Maximum control and flexibility\n",
    "- Best for: Custom workflows, specific requirements\n",
    "\n",
    "**2. Semantic Kernel**\n",
    "- Cross-platform (.NET, Python, Java)\n",
    "- Plugin-based architecture\n",
    "- Best for: Multi-language teams, complex orchestration\n",
    "\n",
    "**3. Microsoft Agent Framework** ‚≠ê (Recommended)\n",
    "- Built specifically for agents\n",
    "- Handles MCP, tools, conversation management\n",
    "- Best for: Most agent use cases, fastest path to production\n",
    "\n",
    "**All three options produce functionally equivalent agents - choose based on your team's needs and existing tech stack.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235e22f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 28: DEMO - Working Application\n",
    "\n",
    "![Slide 28](slides/Slide28.png)\n",
    "\n",
    "|**Speaker Notes** (110 seconds) |\n",
    "|---|\n",
    "|\"So remember Bruno, let's pretend we're Bruno for a second.\"<br><br>**Live Application Demo:**<br><br>1. **Submit Query as Bruno**<br>   - Prompt: \"Here's a picture of my living room. I'm not sure whether I should go with eggshell or semi-gloss. Can you tell me which would work better based on the lighting and layout?\"<br>   - Upload Bruno's living room image<br>   - Submit to Kora<br><br>2. **Watch Agent Process**<br>   - \"So Kora is working on it\"<br>   - Processing image + text<br>   - Making recommendations<br><br>3. **Review Response**<br>   - Kora analyzed the image: \"soft natural light with clean and cozy aesthetic\"<br>   - Lists considerations for eggshell vs semi-gloss<br>   - Recommends eggshell paint<br>   - Asks follow-up: \"Would you like recommendations for the Zava eggshell paint?\"<br><br>4. **Request Product Recommendation**<br>   - \"Recommend a Zava eggshell paint\"<br>   - Behind the scenes: tool call happening<br>   - Kora searches product catalog<br><br>5. **Final Response**<br>   - Product found: Interior Eggshell Paint from Zava<br>   - Price provided: $65.67<br>   - Follow-up: \"Would you like more details on this product or assistance with purchasing?\"<br><br>6. **Ask About Price**<br>   - \"How much is the product?\"<br>   - Kora confirms: $65.67<br><br>**Key Success:** \"What you've just seen here is that the Kora agent has done two tool calls successfully. And very quickly, we have gone from prototyping this core agent and adding that logic into an existing app.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is the CLIMAX demo - show the working application end-to-end\n",
    "- Demonstrate the full user journey as Bruno\n",
    "- Point out the tool calls happening behind the scenes\n",
    "- Emphasize speed from prototype to production\n",
    "- This is what the whole session has been building toward\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Web App Deployment](https://learn.microsoft.com/azure/app-service/)** - Hosting web applications\n",
    "- **[API Integration Patterns](https://learn.microsoft.com/azure/architecture/patterns/)** - Connecting front-end to AI backend\n",
    "- **[Real-time Communication](https://learn.microsoft.com/azure/signalr/)** - Streaming agent responses\n",
    "- **[Authentication & Security](https://learn.microsoft.com/entra/identity/)** - Protecting your application\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Production Application Components:**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Web UI    ‚îÇ ‚Üê User interacts here (Bruno's browser)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  App Server ‚îÇ ‚Üê Flask/FastAPI Python application\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Agent Logic ‚îÇ ‚Üê Kora (exported from AI Toolkit)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  MCP Tools  ‚îÇ ‚Üê Product catalog access\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**What Makes It Production-Ready:**\n",
    "- ‚úÖ Clean separation between UI and agent logic\n",
    "- ‚úÖ Error handling for failed API calls\n",
    "- ‚úÖ Streaming responses for better UX\n",
    "- ‚úÖ Session management for conversations\n",
    "- ‚úÖ Security (authentication, rate limiting)\n",
    "- ‚úÖ Monitoring and logging\n",
    "\n",
    "This is a real application that customers can use, not just a playground experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0826ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 29: Microsoft Foundry Benefits\n",
    "\n",
    "![Slide 29](slides/Slide29.png)\n",
    "\n",
    "|**Speaker Notes** (110 seconds) |\n",
    "|---|\n",
    "|\"With a working app in hand, the final step is getting it out into the world. Serena used GitHub Copilot for Azure to guide her through the deployment process.\"<br><br>\"What's powerful here is that she doesn't need deep Azure expertise, just a clear goal and the right tools. GitHub Copilot for Azure lowers the barrier to entry so developers like Serena can ship with confidence even if they're new to cloud deployment.\"<br><br>**Azure Deployment & Foundry Benefits:**<br><br>\"By deploying to Azure, Zava gains more than just compute. They get enterprise-grade infrastructure, security, governance, and lifecycle management tailored for generative AI workloads.\"<br><br>**Three Key Benefits:**<br><br>1. **\"Built-in security and compliance\"**<br>   - Role-based access controls<br>   - Managed identities and secret handling<br>   - Policy enforcement<br>   - Enterprise-grade governance<br><br>2. **\"Access to centralized model and agent management system\"**<br>   - Version deployments<br>   - Agent registries<br>   - Built-in evaluation pipelines<br><br>3. **\"Scalable infrastructure purpose-built for generative AI\"**<br>   - Elastic model serving (auto-scale based on demand)<br>   - Visibility into cost and usage metrics<br>   - Application remains responsive without over-provisioning<br><br>**Key Message:** \"This is what Microsoft Foundry unlocks for Zava - a production-ready, secure platform that supports not just the launch but the lifecycle of their AI-powered customer experience.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Emphasize enterprise requirements beyond just \"making it work\"\n",
    "- Three pillars: Security, Management, Scale\n",
    "- This is why enterprises choose Azure/Foundry\n",
    "- GitHub Copilot for Azure makes deployment accessible\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Microsoft Foundry](https://learn.microsoft.com/azure/ai-studio/)** - Complete AI development platform\n",
    "- **[Azure Deployment Guide](https://learn.microsoft.com/azure/ai-studio/how-to/deploy-to-cloud)** - Production deployment steps\n",
    "- **[GitHub Copilot for Azure](https://learn.microsoft.com/azure/copilot/)** - AI-assisted Azure management\n",
    "- **[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/)** - Hosting containerized applications\n",
    "- **[Cost Management](https://learn.microsoft.com/azure/cost-management-billing/)** - Understanding and controlling costs\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Why Deploy to Azure / Microsoft Foundry?**\n",
    "\n",
    "**üîí Enterprise Security**\n",
    "- Identity & access management (Entra ID)\n",
    "- Data encryption at rest and in transit\n",
    "- Compliance certifications (SOC 2, HIPAA, etc.)\n",
    "- Network isolation and private endpoints\n",
    "\n",
    "**üìä Management & Governance**\n",
    "- Centralized model deployments\n",
    "- Agent version control\n",
    "- Evaluation pipelines built-in\n",
    "- Audit logs and compliance reporting\n",
    "\n",
    "**‚ö° Scale & Performance**\n",
    "- Auto-scaling based on traffic\n",
    "- Global availability zones\n",
    "- Cost optimization tools\n",
    "- Performance monitoring\n",
    "\n",
    "**üí∞ Cost Control**\n",
    "- Pay only for what you use\n",
    "- Budget alerts and spending limits\n",
    "- Cost analysis and optimization recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c69492",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Slide 30: Observability & Monitoring\n",
    "\n",
    "![Slide 30](slides/Slide30.png)\n",
    "\n",
    "|**Speaker Notes** (110 seconds) |\n",
    "|---|\n",
    "|\"And of course, once Kora is live, Zava needs to keep everything healthy, trustworthy, and measurable. And that's where observability comes in.\"<br><br>\"Once Kora is live, Serena and the Zava team need more than just logs. They need real visibility into how the system is performing.\"<br><br>**Example Scenario:**<br>\"Let's say Bruno, our painting customer, messages Kora and gets no product recommendation in return. Is it a model issue, a back-end error? Did the MCP tool call fail?\"<br><br>**Microsoft Foundry Control Plane:**<br>\"With Foundry control plane, Serena can trace the entire flow end-to-end from the initial user message to the agent's tool call and to the agent's response.\"<br><br>**What is the Control Plane?**<br>\"The Microsoft Foundry control plane is a unified management interface that provides visibility, governance, and control for AI agents, models, and tools across your Foundry enterprise.\"<br><br>**Key Capabilities:**<br>- Drill into latency across steps<br>- Track failures and tool calls<br>- View model responses<br>- See custom metrics (like evaluators to assess friendliness)<br>- End-to-end request tracing<br><br>**Value:** \"This level of observability means Zava can detect problems faster, resolve issues proactively, and deliver a more reliable customer experience.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- Emphasize production monitoring is critical\n",
    "- Observability != just logs\n",
    "- End-to-end tracing is the key differentiator\n",
    "- This closes the loop on Gen AI Ops\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[Azure Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview)** - Application performance monitoring\n",
    "- **[Tracing in Azure AI](https://learn.microsoft.com/azure/ai-studio/how-to/develop/trace-local-sdk)** - End-to-end request tracing\n",
    "- **[Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/)** - Comprehensive monitoring solution\n",
    "- **[Logging Best Practices](https://learn.microsoft.com/azure/ai-studio/how-to/develop/trace-production-sdk)** - Production logging strategies\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**Why Observability Matters:**\n",
    "\n",
    "In production, you need to answer questions like:\n",
    "- ‚ùì Why did this request fail?\n",
    "- ‚ùì Which tool call took 5 seconds?\n",
    "- ‚ùì Is the agent hallucinating in production?\n",
    "- ‚ùì What's our average response time?\n",
    "- ‚ùì How much are we spending per day?\n",
    "\n",
    "**What to Monitor:**\n",
    "\n",
    "**üìä Performance Metrics**\n",
    "- Response latency (total and per-step)\n",
    "- Tool call success rates\n",
    "- Model token usage\n",
    "\n",
    "**‚ö†Ô∏è Error Tracking**\n",
    "- Failed requests and why\n",
    "- Tool call failures\n",
    "- Rate limit hits\n",
    "\n",
    "**üí∞ Cost Metrics**\n",
    "- Tokens consumed per day/week\n",
    "- Cost per conversation\n",
    "- Most expensive operations\n",
    "\n",
    "**‚úÖ Quality Metrics**\n",
    "- Custom evaluators running on sample traffic\n",
    "- User satisfaction signals (thumbs up/down)\n",
    "- Conversation abandonment rates\n",
    "\n",
    "**Pro Tip**: Set up alerts for critical issues (error rate > 5%, latency > 10s, daily cost > budget) so you can respond before users are impacted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009c501",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: Continue Your Journey (38:36 - 40:15)\n",
    "\n",
    "---\n",
    "\n",
    "## Slide 31: Closing & Next Steps\n",
    "\n",
    "![Slide 31](slides/Slide31.png)\n",
    "\n",
    "|**Speaker Notes** (99 seconds) |\n",
    "|---|\n",
    "|\"Today, we walked through what it really looks like to go from idea to impact. From building your first agent to deploying a production-ready AI agent solution backed by Microsoft Foundry.\"<br><br>**Recap What We Covered:**<br>- \"You saw how tools like the AI toolkit, GitHub Copilot, and resources within Microsoft Foundry fit together in a developer-first Gen AI workflow\"<br>- \"And you saw how Serena working under pressure with limited bandwidth was able to still ship something powerful, scalable, and responsible\"<br><br>**Call to Action:**<br>\"Whether you're a solo developer, part of a startup, or scaling enterprise systems, these tools are available to you right now. You can explore the AI toolkit's agent builder inside Visual Studio Code and deploy confidently with Microsoft Foundry.\"<br><br>**Closing Message:**<br>\"The future of AI development isn't about code or creativity alone. It's about how fast you can turn an idea into something real. And now you've got the tools to do it.\"<br><br>**Thank You & Resources:**<br>\"Thank you. You can download today's presentation by scanning the QR code or you can visit aka.ms/microsoft/brk441.\"<br><br>\"And if you're looking for the next steps to advance your AI expertise, check out our apps data dev skills. And you can also scan again for access to the repo for this session.\"|\n",
    "\n",
    "**Speaker Guidance:**\n",
    "- This is the CLOSING - bring energy and inspiration\n",
    "- Recap the full journey (model ‚Üí design ‚Üí evaluate ‚Üí deploy)\n",
    "- Emphasize accessibility of these tools\n",
    "- Clear call to action: try AI Toolkit today\n",
    "- End on an inspiring note about speed to value\n",
    "\n",
    "**Related Resources:**\n",
    "\n",
    "- **[AI Toolkit Download](https://aka.ms/ai-toolkit)** - Install the VS Code extension\n",
    "- **[Session Repository](https://aka.ms/microsoft/brk441)** - Access code samples and resources\n",
    "- **[Microsoft Learn - AI Skills](https://learn.microsoft.com/training/browse/?products=azure-ai-services)** - Free training paths\n",
    "- **[Azure AI Studio](https://ai.azure.com/)** - Microsoft Foundry portal\n",
    "- **[GitHub Models](https://github.com/marketplace/models)** - Free AI models to start experimenting\n",
    "\n",
    "**For Beginners:**\n",
    "\n",
    "**üìö Complete Learning Paths:**\n",
    "\n",
    "1. **[Build AI Solutions with Azure AI](https://learn.microsoft.com/training/paths/create-manage-ai-services-azure/)** - Foundational concepts\n",
    "2. **[Develop Generative AI Solutions](https://learn.microsoft.com/training/paths/develop-ai-solutions-azure-openai/)** - Hands-on with GPT models\n",
    "3. **[Build AI Agents](https://learn.microsoft.com/training/paths/build-copilots-with-semantic-kernel/)** - Agent development\n",
    "\n",
    "**üéØ Quick Start Checklist:**\n",
    "\n",
    "‚úÖ Install AI Toolkit extension in VS Code  \n",
    "‚úÖ Explore GitHub Models (free, no API key needed)  \n",
    "‚úÖ Build your first agent in Agent Builder  \n",
    "‚úÖ Add an MCP tool to your agent  \n",
    "‚úÖ Run evaluations on test data  \n",
    "‚úÖ Export code and deploy to Azure  \n",
    "\n",
    "**You have everything you need to start building AI agents today. What will you create?**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
